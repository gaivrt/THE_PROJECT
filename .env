# Metis Continuum Configuration

# Model settings
MODEL_NAME=deepseek-ai/deepseek-r1-70b
MODEL_DEVICE=cuda

# Server settings
HOST=localhost
PORT=8000

# Thinking loop settings
MIN_THINKING_INTERVAL=0.1
MAX_THINKING_INTERVAL=5.0
INITIAL_THINKING_INTERVAL=1.0

# Memory settings
SHORT_TERM_MEMORY_SIZE=100
LONG_TERM_MEMORY_SIZE=1000
CONTEXT_WINDOW_SIZE=10
MEMORY_CONSOLIDATION_THRESHOLD=0.7

# Emotion settings
EMOTION_DECAY_RATE=0.1
EMOTION_INTENSITY_LIMITS=(-1.0,1.0)

# Desire settings
DESIRE_DECAY_RATE=0.05
MIN_DESIRE_PRIORITY=0.1
MAX_DESIRE_PRIORITY=1.0

# Evaluation settings
EXPRESSION_THRESHOLD=0.7

# LLM Configuration
LLM_TYPE=gemini
GEMINI_API_KEY=AIzaSyAVo5e9K0J93yl145gFUDbaMA_HAPQYH5c
GEMINI_MODEL=gemini-2.0-flash

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=deepseek-r1

# Proxy Configuration
HTTP_PROXY=http://127.0.0.1:10809
HTTPS_PROXY=http://127.0.0.1:10809
SOCKS_PROXY=socks5://127.0.0.1:10808
USE_PROXY=true
